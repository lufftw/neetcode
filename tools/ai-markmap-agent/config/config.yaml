# =============================================================================
# AI Markmap Agent Configuration
# =============================================================================
# All parameters are configurable: models, prompts, agent counts, rounds, etc.
# =============================================================================

# -----------------------------------------------------------------------------
# URL Templates Configuration
# -----------------------------------------------------------------------------
# Configure external URLs for problem links in generated Markmaps
urls:
  # GitHub repository for solutions
  github:
    base: "https://github.com/lufftw/neetcode"
    # Template for solution file links: {solution_file} from problem data
    solution_template: "https://github.com/lufftw/neetcode/blob/main/{solution_file}"
  
  # LeetCode problem links
  leetcode:
    base: "https://leetcode.com"
    # Template for problem page: {slug} from problem data
    problem_template: "https://leetcode.com/problems/{slug}/"
  
  # Link selection logic:
  # 1. If problem has solution_file (non-empty) → use github.solution_template
  # 2. Otherwise → use leetcode.problem_template

# -----------------------------------------------------------------------------
# Data Compression Configuration
# -----------------------------------------------------------------------------
# Token-efficient data transmission to LLM
data_compression:
  enabled: true
  
  # Compression format for problem data
  # Options: "compact_json", "tabular", "minimal"
  format: "compact_json"
  
  # Fields to include in compressed problem data
  # These are the minimal fields needed for Markmap generation
  problem_fields:
    - "id"              # Problem ID (e.g., "0003")
    - "title"           # Problem title
    - "difficulty"      # easy/medium/hard
    - "patterns"        # Algorithm patterns used
    - "has_solution"    # Boolean: true if solution_file exists
    - "topics"          # LeetCode topics
  
  # Fields to extract from ontology (reduce verbosity)
  ontology_summary: true
  
  # Maximum problems per batch (for very large datasets)
  max_problems_per_batch: 200

# -----------------------------------------------------------------------------
# Data Sources Configuration
# -----------------------------------------------------------------------------
# Define which data sources to read from for Markmap generation
# Set enabled: true/false to include/exclude each source
data_sources:
  # Base paths (relative to config/ directory: 3 levels up to reach project root)
  base_paths:
    ontology: "../../../ontology"
    problems: "../../../meta/problems"
    patterns: "../../../meta/patterns"
    roadmaps: "../../../roadmaps"

  # Ontology files - taxonomy definitions
  ontology:
    enabled: true
    files:
      - name: "algorithms"
        path: "algorithms.toml"
        enabled: true
      - name: "api_kernels"
        path: "api_kernels.toml"
        enabled: true
      - name: "data_structures"
        path: "data_structures.toml"
        enabled: true
      - name: "patterns"
        path: "patterns.toml"
        enabled: true
      - name: "families"
        path: "families.toml"
        enabled: true
      - name: "topics"
        path: "topics.toml"
        enabled: true
      - name: "difficulties"
        path: "difficulties.toml"
        enabled: false
      - name: "companies"
        path: "companies.toml"
        enabled: false
      - name: "roadmaps"
        path: "roadmaps.toml"
        enabled: true

  # Problem metadata files (from meta/problems/*.toml)
  problems:
    enabled: true
    
    # Load mode determines HOW to select which problem files to load:
    #
    # "all"     - Load ALL .toml files in the problems directory
    #             Simple but may load more than needed.
    #
    # "list"    - Load ONLY files explicitly listed in 'files' array below.
    #             Use when you want precise control over which problems to process.
    #             Example: files: ["0003_longest_substring.toml", "0076_min_window.toml"]
    #
    # "pattern" - Load files matching glob patterns in 'patterns' array.
    #             Flexible middle ground between "all" and "list".
    #             Example: patterns: ["0003_*.toml", "00[0-7][0-9]_*.toml"]
    #
    load_mode: "all"
    
    # For load_mode: "list" - Explicitly list files to load
    # Example:
    #   files:
    #     - "0003_longest_substring_without_repeating_characters.toml"
    #     - "0076_minimum_window_substring.toml"
    files: []
    
    # For load_mode: "pattern" - Glob patterns to match files
    # Common patterns:
    #   "*.toml"              - All TOML files
    #   "0003_*.toml"         - Only problem 0003
    #   "00[0-9][0-9]_*.toml" - Problems 0000-0099
    patterns:
      - "*.toml"
    
    # Exclude patterns (applied regardless of load_mode)
    exclude:
      - "README.md"

  # Pattern documentation directories
  patterns:
    enabled: true
    directories:
      - name: "sliding_window"
        path: "sliding_window"
        enabled: true
        config_file: "_config.toml"
      - name: "two_pointers"
        path: "two_pointers"
        enabled: true
        config_file: "_config.toml"

  # Roadmap learning paths
  roadmaps:
    enabled: true
    files:
      - name: "sliding_window_path"
        path: "sliding_window_path.toml"
        enabled: true
      - name: "neetcode_150"
        path: "neetcode_150.toml"
        enabled: false
      - name: "blind_75"
        path: "blind_75.toml"
        enabled: false

# -----------------------------------------------------------------------------
# Prompt Mode Configuration
# -----------------------------------------------------------------------------
# Choose between static (pre-defined) prompts or dynamic (AI-generated) prompts
prompt_mode:
  # "static" = Use pre-defined prompts in prompts/ directory
  # "dynamic" = Generate prompts using AI at runtime
  mode: "static"
  
  # Model to use for generating dynamic prompts (only used when mode="dynamic")
  generator_model: "gpt-4"
  
  # Meta-prompts for dynamic generation
  meta_prompts:
    persona_generator: "prompts/meta/generate_optimizer_persona.md"
    behavior_generator: "prompts/meta/generate_optimizer_behavior.md"
    role_suggester: "prompts/meta/suggest_optimizer_roles.md"
  
  # Cache generated prompts (recommended for consistency across runs)
  cache_generated: true
  cache_dir: "prompts/generated"

# -----------------------------------------------------------------------------
# Model Configuration
# -----------------------------------------------------------------------------
models:
  # Planners - Structure Specification generators
  generalist_planner:
    en:
      model: "gpt-4o"
      persona_prompt: "prompts/planners/generalist_planner_persona.md"
      behavior_prompt: "prompts/planners/generalist_planner_behavior.md"
      temperature: 0.7
      max_tokens: 4096
    zh:
      model: "gpt-4o"
      persona_prompt: "prompts/planners/generalist_planner_persona.md"
      behavior_prompt: "prompts/planners/generalist_planner_behavior.md"
      temperature: 0.7
      max_tokens: 4096

  specialist_planner:
    en:
      model: "gpt-4o"
      persona_prompt: "prompts/planners/specialist_planner_persona.md"
      behavior_prompt: "prompts/planners/specialist_planner_behavior.md"
      temperature: 0.5
      max_tokens: 4096
    zh:
      model: "gpt-4o"
      persona_prompt: "prompts/planners/specialist_planner_persona.md"
      behavior_prompt: "prompts/planners/specialist_planner_behavior.md"
      temperature: 0.5
      max_tokens: 4096

  # Content Strategists - Three distinct expert perspectives for discussion
  # Uses Structure Spec (YAML), not Markdown
  content_strategist:
    - id: "architect_strategist"
      name: "Architecture Strategist"
      model: "gpt-4"
      persona_prompt: "prompts/strategists/architect_strategist_persona.md"
      behavior_prompt: "prompts/strategists/architect_strategist_behavior.md"
      temperature: 0.6
      max_tokens: 4096
      focus: "structure_modularity"
    
    - id: "professor_strategist"
      name: "Academic Strategist"
      model: "gpt-4"
      persona_prompt: "prompts/strategists/professor_strategist_persona.md"
      behavior_prompt: "prompts/strategists/professor_strategist_behavior.md"
      temperature: 0.6
      max_tokens: 4096
      focus: "correctness_completeness"
    
    - id: "ux_strategist"
      name: "UX Strategist"
      model: "gpt-4"
      persona_prompt: "prompts/strategists/ux_strategist_persona.md"
      behavior_prompt: "prompts/strategists/ux_strategist_behavior.md"
      temperature: 0.7
      max_tokens: 4096
      focus: "user_experience"

  # Integrator - Consolidates strategist suggestions
  integrator:
    model: "gpt-4o"
    persona_prompt: "prompts/integrator/integrator_persona.md"
    behavior_prompt: "prompts/integrator/integrator_behavior.md"
    temperature: 0.5
    max_tokens: 4096

  # Evaluators - Structure Specification evaluation
  # Uses Structure Spec (YAML), not Markdown
  evaluator:
    - id: "structure_evaluator"
      name: "Structure Evaluator"
      model: "gpt-4"
      behavior_prompt: "prompts/evaluators/structure_evaluator_behavior.md"
      temperature: 0.4
      max_tokens: 4096
      criteria:
        - "logical_organization"
        - "appropriate_depth"
        - "balanced_sections"
    
    - id: "content_evaluator"
      name: "Content Evaluator"
      model: "gpt-4"
      behavior_prompt: "prompts/evaluators/content_evaluator_behavior.md"
      temperature: 0.4
      max_tokens: 4096
      criteria:
        - "coverage"
        - "learning_progression"
        - "practical_value"

  # Writer - Final Markmap generation
  # Responsible for:
  #   1. Applying evaluator feedback and suggestions
  #   2. Generating proper links (GitHub/LeetCode)
  #   3. Applying Markmap formatting (checkboxes, KaTeX, fold, etc.)
  writer:
    model: "gpt-4o"  # 128K context window
    persona_prompt: "prompts/writer/writer_persona.md"
    behavior_prompt: "prompts/writer/writer_behavior.md"
    format_guide: "prompts/writer/markmap_format_guide.md"
    temperature: 0.5
    max_tokens: 8192

  # Translator - For translate mode languages
  translator:
    model: "gpt-4"
    temperature: 0.3
    max_tokens: 8192

  # Compressor - For summarizing long content (use cheaper model)
  compressor:
    model: "gpt-3.5-turbo"
    behavior_prompt: "prompts/compressor/compressor_behavior.md"
    temperature: 0.3
    max_tokens: 2048

# -----------------------------------------------------------------------------
# Workflow Configuration
# -----------------------------------------------------------------------------
workflow:
  # Maximum discussion rounds for strategists
  max_discussion_rounds: 3
  
  # Consensus threshold (0.0-1.0)
  # If strategists agree above this threshold, discussion ends early
  consensus_threshold: 0.8
  
  # Token threshold to trigger compression
  max_tokens_before_compress: 8000
  
  # Enable parallel structure generation (Phase 1)
  parallel_generation: true
  
  # ---------------------------------------------------------------------------
  # Post-Processing Settings (applied by program, not LLM)
  # ---------------------------------------------------------------------------
  post_processing:
    # Text replacements applied to final output
    # These are done by code, reducing LLM prompt burden
    text_replacements:
      # Replace "LC" abbreviation with full "LeetCode"
      - pattern: "\\bLC[-\\s]?(\\d+)"
        replacement: "LeetCode \\1"
      # Ensure consistent spacing
      - pattern: "LeetCode(\\d+)"
        replacement: "LeetCode \\1"

# -----------------------------------------------------------------------------
# Debug Output Configuration
# -----------------------------------------------------------------------------
# Configure intermediate output saving for debugging and verification
debug_output:
  # Master switch for debug outputs
  enabled: true
  
  # Base directory for debug outputs
  output_dir: "outputs/debug"
  
  # Save LLM inputs and outputs for debugging
  # This saves the FULL prompt sent to each LLM call
  llm_calls:
    enabled: true                    # Save LLM call details
    save_input: true                 # Save full prompt/input to LLM
    save_output: true                # Save LLM response
    save_as_single_file: false       # true = one file per call, false = append to log
    format: "md"                     # "md" for readable, "json" for structured
  
  # Save outputs for each phase
  phases:
    # Phase 1: Structure generation
    baseline:
      enabled: true
      save_each_generator: true    # Save output from each planner
    
    # Phase 2: Strategy discussion rounds
    optimization:
      enabled: true
      save_each_round: true        # Save structure after each round
      save_strategist_suggestions: true  # Save each strategist's suggestions
      save_integrator_output: true # Save integrator's consolidated output
    
    # Phase 3: Evaluation
    evaluation:
      enabled: true
      save_evaluations: true       # Save each evaluator's assessment
      save_final_consensus: true   # Save final consensus
    
    # Phase 4: Writer
    writer:
      enabled: true
      save_writer_input: true      # Save input to writer (structure + feedback)
      save_writer_output: true     # Save writer's final output
    
    # Phase 5: Translation
    translation:
      enabled: true
      save_before_translation: true  # Save English version before translation
      save_after_translation: true   # Save translated versions
    
    # Phase 6: Post-processing
    post_processing:
      enabled: true
      save_before_processing: true   # Save before LC → LeetCode replacement
      save_after_processing: true    # Save after post-processing
  
  # Output format settings
  format:
    # Include timestamps in filenames
    include_timestamp: true
    # Include phase number in filename (e.g., "01_baseline_generalist_en.md")
    include_phase_number: true
    # Filename template: {phase}_{agent}_{lang}_{timestamp}.md
    template: "{phase_num:02d}_{phase}_{agent}_{lang}"

# -----------------------------------------------------------------------------
# Memory Configuration
# -----------------------------------------------------------------------------
memory:
  stm:
    enabled: true
    max_items: 50
  
  ltm:
    enabled: true
    vector_store: "chromadb"
    collection_name: "markmap_decisions"
    embedding_model: "text-embedding-3-small"
    chromadb:
      persist_directory: "./data/chromadb"
    retrieval:
      k: 5
      score_threshold: 0.7

# -----------------------------------------------------------------------------
# Output Configuration
# -----------------------------------------------------------------------------
output:
  # Intermediate outputs (during processing)
  save_intermediate: true
  intermediate_dir: "outputs/intermediate"
  
  # Final output directories (relative to project root or absolute)
  final_dirs:
    markdown: "../../docs/mindmaps"        # .md files
    html: "../../docs/pages/mindmaps"      # .html files
  
  # Naming convention
  # Output files: neetcode_{type}_ai_{lang}.md / .html
  naming:
    prefix: "neetcode"
    
    # Languages to generate
    # Each language can use one of two modes:
    #   "generate"  - Run full pipeline from scratch (slow)
    #   "translate" - Translate from another language's output (fast)
    #
    languages:
      en:
        enabled: true
        mode: "generate"       # Primary language: run full pipeline
      
      zh-TW:
        enabled: true
        mode: "translate"      # Translate from English (fast)
        source_lang: "en"      # Source language to translate from
        translator_model: "gpt-4"
    
    # Output types
    types:
      general:
        description: "Broad understanding, knowledge organization"
        generator: "generalist"
    
    # File naming template: {prefix}_{type}_ai_{lang}.{ext}
    # Examples:
    #   neetcode_general_ai_en.md
    #   neetcode_general_ai_zh-TW.html
    template: "{prefix}_{type}_ai_{lang}"
    
    # Intermediate files
    round: "{prefix}_{type}_ai_{lang}_round_{n}.md"
  
  html:
    template: "templates/markmap.html"
    include_styles: true
    include_scripts: true
    title: "AI Generated Markmap"

# -----------------------------------------------------------------------------
# API Configuration
# -----------------------------------------------------------------------------
api:
  openai:
    api_key: "${OPENAI_API_KEY}"
    organization: "${OPENAI_ORG_ID}"
    base_url: null
  retry:
    max_retries: 3
    retry_delay: 1.0
    exponential_backoff: true

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/ai_markmap_agent.log"
  console: true
  log_llm_calls: false

# -----------------------------------------------------------------------------
# Development Configuration
# -----------------------------------------------------------------------------
dev:
  debug: false
  use_mock_llm: false
  langgraph_studio:
    enabled: true
    port: 8123

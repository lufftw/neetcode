# =============================================================================
# AI Markmap Agent Configuration
# =============================================================================
# All parameters are configurable: models, prompts, agent counts, rounds, etc.
# =============================================================================

# -----------------------------------------------------------------------------
# URL Templates Configuration
# -----------------------------------------------------------------------------
# Configure external URLs for problem links in generated Markmaps
urls:
  # GitHub repository for solutions
  github:
    base: "https://github.com/lufftw/neetcode"
    # Template for solution file links: {solution_file} from problem data
    solution_template: "https://github.com/lufftw/neetcode/blob/main/{solution_file}"
  
  # LeetCode problem links
  leetcode:
    base: "https://leetcode.com"
    # Template for problem page: {slug} from problem data
    problem_template: "https://leetcode.com/problems/{slug}/"
  
  # Link selection logic:
  # 1. If problem has solution_file (non-empty) → use github.solution_template
  # 2. Otherwise → use leetcode.problem_template

# -----------------------------------------------------------------------------
# Data Compression Configuration
# -----------------------------------------------------------------------------
# Token-efficient data transmission to LLM
data_compression:
  enabled: true
  
  # Compression format for problem data
  # Options: "compact_json", "tabular", "minimal"
  format: "compact_json"
  
  # Fields to include in compressed problem data
  # These are the minimal fields needed for Markmap generation
  problem_fields:
    - "id"              # Problem ID (e.g., "0003")
    - "title"           # Problem title
    - "difficulty"      # easy/medium/hard
    - "patterns"        # Algorithm patterns used
    - "has_solution"    # Boolean: true if solution_file exists
    - "topics"          # LeetCode topics
  
  # Fields to extract from ontology (reduce verbosity)
  ontology_summary: true
  
  # Maximum problems per batch (for very large datasets)
  max_problems_per_batch: 200

# -----------------------------------------------------------------------------
# Data Sources Configuration
# -----------------------------------------------------------------------------
# Define which data sources to read from for Markmap generation
# Set enabled: true/false to include/exclude each source
data_sources:
  # Base paths (relative to project root)
  base_paths:
    ontology: "../../ontology"
    problems: "../../meta/problems"
    patterns: "../../meta/patterns"
    roadmaps: "../../roadmaps"

  # Ontology files - taxonomy definitions
  ontology:
    enabled: true
    files:
      - name: "algorithms"
        path: "algorithms.toml"
        enabled: true
      - name: "api_kernels"
        path: "api_kernels.toml"
        enabled: true
      - name: "data_structures"
        path: "data_structures.toml"
        enabled: true
      - name: "patterns"
        path: "patterns.toml"
        enabled: true
      - name: "families"
        path: "families.toml"
        enabled: true
      - name: "topics"
        path: "topics.toml"
        enabled: true
      - name: "difficulties"
        path: "difficulties.toml"
        enabled: false
      - name: "companies"
        path: "companies.toml"
        enabled: false
      - name: "roadmaps"
        path: "roadmaps.toml"
        enabled: true

  # Problem metadata files (from meta/problems/*.toml)
  problems:
    enabled: true
    
    # Load mode determines HOW to select which problem files to load:
    #
    # "all"     - Load ALL .toml files in the problems directory
    #             Simple but may load more than needed.
    #
    # "list"    - Load ONLY files explicitly listed in 'files' array below.
    #             Use when you want precise control over which problems to process.
    #             Example: files: ["0003_longest_substring.toml", "0076_min_window.toml"]
    #
    # "pattern" - Load files matching glob patterns in 'patterns' array.
    #             Flexible middle ground between "all" and "list".
    #             Example: patterns: ["0003_*.toml", "00[0-7][0-9]_*.toml"]
    #
    load_mode: "all"
    
    # For load_mode: "list" - Explicitly list files to load
    # Example:
    #   files:
    #     - "0003_longest_substring_without_repeating_characters.toml"
    #     - "0076_minimum_window_substring.toml"
    files: []
    
    # For load_mode: "pattern" - Glob patterns to match files
    # Common patterns:
    #   "*.toml"              - All TOML files
    #   "0003_*.toml"         - Only problem 0003
    #   "00[0-9][0-9]_*.toml" - Problems 0000-0099
    patterns:
      - "*.toml"
    
    # Exclude patterns (applied regardless of load_mode)
    exclude:
      - "README.md"

  # Pattern documentation directories
  patterns:
    enabled: true
    directories:
      - name: "sliding_window"
        path: "sliding_window"
        enabled: true
        config_file: "_config.toml"
      - name: "two_pointers"
        path: "two_pointers"
        enabled: true
        config_file: "_config.toml"

  # Roadmap learning paths
  roadmaps:
    enabled: true
    files:
      - name: "sliding_window_path"
        path: "sliding_window_path.toml"
        enabled: true
      - name: "neetcode_150"
        path: "neetcode_150.toml"
        enabled: false
      - name: "blind_75"
        path: "blind_75.toml"
        enabled: false

# -----------------------------------------------------------------------------
# Prompt Mode Configuration
# -----------------------------------------------------------------------------
# Choose between static (pre-defined) prompts or dynamic (AI-generated) prompts
prompt_mode:
  # "static" = Use pre-defined prompts in prompts/ directory
  # "dynamic" = Generate prompts using AI at runtime
  mode: "static"
  
  # Model to use for generating dynamic prompts (only used when mode="dynamic")
  generator_model: "gpt-5.2"
  
  # Meta-prompts for dynamic generation
  meta_prompts:
    persona_generator: "prompts/meta/generate_optimizer_persona.md"
    behavior_generator: "prompts/meta/generate_optimizer_behavior.md"
    role_suggester: "prompts/meta/suggest_optimizer_roles.md"
  
  # Cache generated prompts (recommended for consistency across runs)
  cache_generated: true
  cache_dir: "prompts/generated"

# -----------------------------------------------------------------------------
# Model Configuration
# -----------------------------------------------------------------------------
models:
  # Generalist - Broad understanding, knowledge organization
  generalist:
    en:
      model: "gpt-5.2"
      persona_prompt: "prompts/generators/generalist_persona.md"
      behavior_prompt: "prompts/generators/generalist_behavior.md"
      temperature: 0.7
      max_tokens: 4096
    zh:
      model: "gpt-5.2"
      persona_prompt: "prompts/generators/generalist_persona.md"
      behavior_prompt: "prompts/generators/generalist_behavior.md"
      temperature: 0.7
      max_tokens: 4096

  # Specialist - Engineering details, structural rigor
  specialist:
    en:
      model: "gpt-5.2"
      persona_prompt: "prompts/generators/specialist_persona.md"
      behavior_prompt: "prompts/generators/specialist_behavior.md"
      temperature: 0.5
      max_tokens: 4096
    zh:
      model: "gpt-5.2"
      persona_prompt: "prompts/generators/specialist_persona.md"
      behavior_prompt: "prompts/generators/specialist_behavior.md"
      temperature: 0.5
      max_tokens: 4096

  # Optimizers - Three distinct expert perspectives for debate
  optimizer:
    # Top-tier Software Architect (Dr. Alexander Chen)
    - id: "optimizer_architect"
      name: "The Software Architect"
      persona_name: "Dr. Alexander Chen"
      model: "gpt-5.1"
      persona_prompt: "prompts/optimizers/optimizer_architect_persona.md"
      behavior_prompt: "prompts/optimizers/optimizer_architect_behavior.md"
      temperature: 0.6
      max_tokens: 4096
      focus: "architecture_modularity"
      # For dynamic mode:
      dynamic_config:
        role_description: "Top-tier Software Architect"
        focus_area: "system design, modularity, clean architecture, design patterns"
        perspective: "structural and organizational excellence"
    
    # Senior Algorithm Professor (Prof. David Knuth Jr.)
    - id: "optimizer_professor"
      name: "The Algorithm Professor"
      persona_name: "Prof. David Knuth Jr."
      model: "gpt-5.1"
      persona_prompt: "prompts/optimizers/optimizer_professor_persona.md"
      behavior_prompt: "prompts/optimizers/optimizer_professor_behavior.md"
      temperature: 0.6
      max_tokens: 4096
      focus: "correctness_completeness"
      # For dynamic mode:
      dynamic_config:
        role_description: "Distinguished Algorithm Professor and Computer Scientist"
        focus_area: "algorithms, data structures, computational complexity, formal methods"
        perspective: "academic rigor and correctness"
    
    # Senior Technical Architect / API Designer (James Patterson)
    - id: "optimizer_apidesigner"
      name: "The Technical API Architect"
      persona_name: "James Patterson"
      model: "gpt-5.1"
      persona_prompt: "prompts/optimizers/optimizer_apidesigner_persona.md"
      behavior_prompt: "prompts/optimizers/optimizer_apidesigner_behavior.md"
      temperature: 0.7
      max_tokens: 4096
      focus: "developer_experience"
      # For dynamic mode:
      dynamic_config:
        role_description: "Senior Technical Architect and API Designer"
        focus_area: "API design, developer experience, documentation, interface patterns"
        perspective: "usability and developer-centric design"

  # Summarizer - Consolidates each round's discussion
  summarizer:
    model: "gpt-5.2"
    persona_prompt: "prompts/summarizer/summarizer_persona.md"
    behavior_prompt: "prompts/summarizer/summarizer_behavior.md"
    temperature: 0.5
    max_tokens: 4096

  # Judges - Final evaluation and selection
  judges:
    - id: "judge_quality"
      name: "Quality Judge"
      model: "gpt-4"
      persona_prompt: "prompts/judges/judge_quality_persona.md"
      behavior_prompt: "prompts/judges/judge_quality_behavior.md"
      temperature: 0.4
      max_tokens: 4096
      criteria:
        - "structure_quality"
        - "naming_consistency"
        - "technical_accuracy"
    
    - id: "judge_completeness"
      name: "Completeness Judge"
      model: "gpt-4"
      persona_prompt: "prompts/judges/judge_completeness_persona.md"
      behavior_prompt: "prompts/judges/judge_completeness_behavior.md"
      temperature: 0.4
      max_tokens: 4096
      criteria:
        - "knowledge_coverage"
        - "practical_value"
        - "depth_balance"

  # Compressor - For summarizing long content (use cheaper model)
  compressor:
    model: "gpt-3.5-turbo"
    behavior_prompt: "prompts/compressor/compressor_behavior.md"
    temperature: 0.3
    max_tokens: 2048

# -----------------------------------------------------------------------------
# Workflow Configuration
# -----------------------------------------------------------------------------
workflow:
  # Number of optimization rounds
  # NOTE: Recommended setting is 3 rounds for production quality
  # Currently set to 1 for faster iteration during development
  optimization_rounds: 1  # Production: 3
  
  # Number of optimizers (must match models.optimizer count)
  optimizer_count: 3
  
  # Number of judges (must match models.judges count)
  judge_count: 2
  
  # Token threshold to trigger compression
  max_tokens_before_compress: 8000
  
  # Enable parallel baseline generation (Phase 1)
  parallel_baseline_generation: true
  
  # Enable debate between judges
  # NOTE: Recommended true for production quality
  enable_debate: false  # Production: true
  
  # Maximum debate rounds
  # NOTE: Recommended 2 for production
  max_debate_rounds: 1  # Production: 2

# -----------------------------------------------------------------------------
# Memory Configuration
# -----------------------------------------------------------------------------
memory:
  stm:
    enabled: true
    max_items: 50
  
  ltm:
    enabled: true
    vector_store: "chromadb"
    collection_name: "markmap_decisions"
    embedding_model: "text-embedding-3-small"
    chromadb:
      persist_directory: "./data/chromadb"
    retrieval:
      k: 5
      score_threshold: 0.7

# -----------------------------------------------------------------------------
# Output Configuration
# -----------------------------------------------------------------------------
output:
  # Intermediate outputs (during processing)
  save_intermediate: true
  intermediate_dir: "outputs/intermediate"
  
  # Final output directories (relative to project root or absolute)
  final_dirs:
    markdown: "../../docs/mindmaps"        # .md files
    html: "../../docs/pages/mindmaps"      # .html files
  
  # Naming convention - generates 4 final outputs (2 types × 2 languages)
  # Output files: neetcode_{type}_ai_{lang}.md / .html
  naming:
    prefix: "neetcode"
    
    # Languages to generate
    # Each language can use one of two modes:
    #   "generate"  - Run full optimization pipeline from scratch (slow)
    #   "translate" - Translate from another language's output (fast, DEFAULT for non-primary)
    #
    languages:
      en:
        enabled: true
        mode: "generate"       # Primary language: run full pipeline
      
      zh-TW:
        enabled: true
        mode: "translate"      # DEFAULT: translate from English (fast)
        # mode: "generate"     # Alternative: run full pipeline independently (slow)
        source_lang: "en"      # Source language to translate from
        translator_model: "gpt-4o"  # Model for translation
    
    # Output types
    types:
      general:
        description: "Broad understanding, knowledge organization"
        generator: "generalist"
      specialist:
        description: "Engineering details, structural rigor"
        generator: "specialist"
    
    # File naming template: {prefix}_{type}_ai_{lang}.{ext}
    # Examples:
    #   neetcode_general_ai_en.md
    #   neetcode_general_ai_zh-TW.html
    #   neetcode_specialist_ai_en.md
    #   neetcode_specialist_ai_zh-TW.html
    template: "{prefix}_{type}_ai_{lang}"
    
    # Intermediate files
    round: "{prefix}_{type}_ai_{lang}_round_{n}.md"
  
  html:
    template: "templates/markmap.html"
    include_styles: true
    include_scripts: true
    title: "AI Generated Markmap"

# -----------------------------------------------------------------------------
# API Configuration
# -----------------------------------------------------------------------------
api:
  openai:
    api_key: "${OPENAI_API_KEY}"
    organization: "${OPENAI_ORG_ID}"
    base_url: null
  retry:
    max_retries: 3
    retry_delay: 1.0
    exponential_backoff: true

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/ai_markmap_agent.log"
  console: true
  log_llm_calls: false

# -----------------------------------------------------------------------------
# Development Configuration
# -----------------------------------------------------------------------------
dev:
  debug: false
  use_mock_llm: false
  langgraph_studio:
    enabled: true
    port: 8123

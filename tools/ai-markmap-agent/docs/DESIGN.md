# AI Markmap Agent - Technical Design Document

> æœ¬æ–‡ä»¶è©³ç´°èªªæ˜ç³»çµ±çš„æŠ€è¡“è¨­è¨ˆæ±ºç­–ã€LangGraph å¯¦ä½œç´°ç¯€ã€ä»¥åŠå„æ¨¡çµ„çš„äº’å‹•æ–¹å¼ã€‚

## ç›®éŒ„

1. [è¨­è¨ˆåŸå‰‡](#è¨­è¨ˆåŸå‰‡)
2. [LangGraph æ ¸å¿ƒæ¦‚å¿µ](#langgraph-æ ¸å¿ƒæ¦‚å¿µ)
3. [State è¨­è¨ˆ](#state-è¨­è¨ˆ)
4. [Graph çµæ§‹](#graph-çµæ§‹)
5. [Agent è¨­è¨ˆæ¨¡å¼](#agent-è¨­è¨ˆæ¨¡å¼)
6. [è¨˜æ†¶ç³»çµ±æ¶æ§‹](#è¨˜æ†¶ç³»çµ±æ¶æ§‹)
7. [éŒ¯èª¤è™•ç†ç­–ç•¥](#éŒ¯èª¤è™•ç†ç­–ç•¥)
8. [æ•ˆèƒ½å„ªåŒ–](#æ•ˆèƒ½å„ªåŒ–)

---

## è¨­è¨ˆåŸå‰‡

### 1. å¯é…ç½®æ€§ (Configurability)
- æ‰€æœ‰åƒæ•¸çš†å¯é€é YAML é…ç½®
- æ”¯æ´ç’°å¢ƒè®Šæ•¸æ’å€¼ (`${VAR_NAME}`)
- ç†±é‡è¼‰é…ç½®ï¼ˆé–‹ç™¼æ¨¡å¼ï¼‰

### 2. å¯æ“´å±•æ€§ (Extensibility)
- æ–°å¢ Agent åªéœ€å®šç¾©é…ç½®èˆ‡ Prompt
- æ”¯æ´è‡ªè¨‚ Vector Store å¯¦ä½œ
- æ¨¡çµ„åŒ–è¨­è¨ˆä¾¿æ–¼æ›¿æ›å…ƒä»¶

### 3. å¯è§€æ¸¬æ€§ (Observability)
- å®Œæ•´çš„æ—¥èªŒè¨˜éŒ„
- LangGraph Studio å¯è¦–åŒ–
- Checkpoint æ”¯æ´ä¸­æ–·æ¢å¾©

### 4. å¯æ¸¬è©¦æ€§ (Testability)
- Mock LLM æ”¯æ´å–®å…ƒæ¸¬è©¦
- ç¨ç«‹æ¨¡çµ„å¯å–®ç¨æ¸¬è©¦
- æ•´åˆæ¸¬è©¦è¦†è“‹å®Œæ•´æµç¨‹

---

## LangGraph æ ¸å¿ƒæ¦‚å¿µ

### State + Graph ç¯„å¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LangGraph æ¶æ§‹                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   State (TypedDict)          Graph (StateGraph)                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ â€¢ metadata      â”‚        â”‚ Nodes:                      â”‚   â”‚
â”‚   â”‚ â€¢ markmaps      â”‚ â”€â”€â”€â”€â”€â”€ â”‚ â€¢ generate_generalist_en    â”‚   â”‚
â”‚   â”‚ â€¢ discussions   â”‚        â”‚ â€¢ generate_generalist_zh    â”‚   â”‚
â”‚   â”‚ â€¢ round_info    â”‚        â”‚ â€¢ optimize                  â”‚   â”‚
â”‚   â”‚ â€¢ memory        â”‚        â”‚ â€¢ summarize                 â”‚   â”‚
â”‚   â”‚ â€¢ final_output  â”‚        â”‚ â€¢ evaluate                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚                             â”‚   â”‚
â”‚                              â”‚ Edges:                      â”‚   â”‚
â”‚                              â”‚ â€¢ START â†’ generators        â”‚   â”‚
â”‚                              â”‚ â€¢ generators â†’ collect      â”‚   â”‚
â”‚                              â”‚ â€¢ collect â†’ optimize (loop) â”‚   â”‚
â”‚                              â”‚ â€¢ optimize â†’ evaluate       â”‚   â”‚
â”‚                              â”‚ â€¢ evaluate â†’ END            â”‚   â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### é—œéµ API

| API | ç”¨é€” | ç¯„ä¾‹ |
|-----|------|------|
| `StateGraph(State)` | å»ºç«‹æœ‰ç‹€æ…‹çš„ Graph | `graph = StateGraph(MarkmapState)` |
| `add_node(name, func)` | æ–°å¢ç¯€é» | `graph.add_node("optimize", optimize_fn)` |
| `add_edge(from, to)` | æ–°å¢é‚Š | `graph.add_edge("a", "b")` |
| `add_conditional_edges()` | æ¢ä»¶è·¯ç”± | æ ¹æ“šç‹€æ…‹æ±ºå®šä¸‹ä¸€æ­¥ |
| `compile(checkpointer)` | ç·¨è­¯ä¸¦å•Ÿç”¨æŒä¹…åŒ– | `graph.compile(checkpointer=MemorySaver())` |

---

## State è¨­è¨ˆ

### MarkmapState å®šç¾©

```python
from typing import TypedDict, List, Optional, Annotated
from langgraph.graph.message import add_messages

class MarkmapState(TypedDict):
    """
    å…±äº«ç‹€æ…‹ - åœ¨æ‰€æœ‰ç¯€é»é–“å‚³é
    
    è¨­è¨ˆåŸå‰‡ï¼š
    1. ä¸å¯è®Šæ€§ï¼šæ¯æ¬¡æ›´æ–°è¿”å›æ–°å­—å…¸
    2. å¯è¿½è¹¤æ€§ï¼šä¿ç•™å®Œæ•´æ­·å²
    3. æœ€å°åŒ–ï¼šåƒ…åŒ…å«å¿…è¦è³‡è¨Š
    """
    
    # ===== è¼¸å…¥æ•¸æ“š =====
    metadata: Optional[dict]           # å…¨é‡ metadataï¼ˆåƒ…é¦–æ¬¡ï¼‰
    ontology: Optional[dict]           # ontology æ•¸æ“š
    
    # ===== ç¬¬ä¸€éšæ®µç”¢ç‰© =====
    markmap_general_en: Optional[str]
    markmap_general_zh: Optional[str]
    markmap_specialist_en: Optional[str]
    markmap_specialist_zh: Optional[str]
    
    # ===== æµç¨‹ç‹€æ…‹ =====
    current_round: int
    current_markmaps: List[str]
    
    # ===== è¨è«–ç´€éŒ„ =====
    # ä½¿ç”¨ add_messages reducer è‡ªå‹•ç´¯ç©
    discussion_history: Annotated[List[dict], add_messages]
    round_summaries: List[str]
    
    # ===== å£“ç¸®å…§å®¹ =====
    compressed_discussion: Optional[str]
    compressed_metadata: Optional[str]
    
    # ===== è©•æ–·çµæœ =====
    candidate_markmaps: List[dict]
    judge_evaluations: List[dict]
    final_selection: Optional[str]
    
    # ===== è¼¸å‡º =====
    final_html: Optional[str]
    
    # ===== è¨˜æ†¶ =====
    stm: dict
    ltm_context: Optional[str]
```

### Reducer æ©Ÿåˆ¶

LangGraph ä½¿ç”¨ Reducer è™•ç†ç‹€æ…‹æ›´æ–°ï¼š

```python
# add_messages reducer ç¯„ä¾‹
# è‡ªå‹•å°‡æ–°è¨Šæ¯ç´¯ç©åˆ°æ­·å²ä¸­

# ç¯€é»è¿”å›ï¼š
return {"discussion_history": [new_message]}

# State æ›´æ–°å¾Œï¼š
# discussion_history = [old_msg1, old_msg2, new_message]
```

---

## Graph çµæ§‹

### å®Œæ•´ Graph å®šç¾©

```python
from langgraph.graph import StateGraph, START, END

def build_graph():
    graph = StateGraph(MarkmapState)
    
    # ===== Phase 1: Baseline Generation =====
    graph.add_node("gen_general_en", generate_generalist_en)
    graph.add_node("gen_general_zh", generate_generalist_zh)
    graph.add_node("gen_specialist_en", generate_specialist_en)
    graph.add_node("gen_specialist_zh", generate_specialist_zh)
    graph.add_node("collect", collect_baselines)
    
    # Parallel edges from START
    graph.add_edge(START, "gen_general_en")
    graph.add_edge(START, "gen_general_zh")
    graph.add_edge(START, "gen_specialist_en")
    graph.add_edge(START, "gen_specialist_zh")
    
    # All generators â†’ collect
    graph.add_edge("gen_general_en", "collect")
    graph.add_edge("gen_general_zh", "collect")
    graph.add_edge("gen_specialist_en", "collect")
    graph.add_edge("gen_specialist_zh", "collect")
    
    # ===== Phase 2: Optimization Loop =====
    graph.add_node("compress", compress_if_needed)
    graph.add_node("optimize", run_optimization)
    graph.add_node("summarize", summarize_round)
    
    graph.add_edge("collect", "compress")
    graph.add_edge("compress", "optimize")
    graph.add_edge("optimize", "summarize")
    
    # Conditional: continue or evaluate
    graph.add_conditional_edges(
        "summarize",
        should_continue,
        {"continue": "compress", "evaluate": "evaluate"}
    )
    
    # ===== Phase 3: Final Evaluation =====
    graph.add_node("evaluate", run_evaluation)
    graph.add_node("convert", convert_to_html)
    
    graph.add_edge("evaluate", "convert")
    graph.add_edge("convert", END)
    
    return graph.compile(checkpointer=MemorySaver())
```

### æ¢ä»¶è·¯ç”±é‚è¼¯

```python
def should_continue(state: MarkmapState) -> Literal["continue", "evaluate"]:
    """
    æ±ºå®šæ˜¯å¦ç¹¼çºŒå„ªåŒ–
    
    æ¢ä»¶ï¼š
    1. æœªé”æœ€å¤§è¼ªæ•¸
    2. ä¸Šè¼ªæœ‰é¡¯è‘—æ”¹é€²ï¼ˆå¯é¸ï¼‰
    """
    config = load_config()
    max_rounds = config["workflow"]["optimization_rounds"]
    
    if state["current_round"] < max_rounds:
        return "continue"
    return "evaluate"
```

---

## Agent è¨­è¨ˆæ¨¡å¼

### Base Agent æŠ½è±¡

```python
from abc import ABC, abstractmethod
from langchain_core.messages import HumanMessage

class BaseAgent(ABC):
    """æ‰€æœ‰ Agent çš„åŸºé¡"""
    
    def __init__(self, config: dict):
        self.config = config
        self.model = self._init_model()
        self.prompt = self._load_prompt()
    
    @abstractmethod
    def _init_model(self):
        """åˆå§‹åŒ– LLM"""
        pass
    
    def _load_prompt(self) -> str:
        """è¼‰å…¥ Prompt æ¨¡æ¿"""
        with open(self.config["prompt_path"], "r") as f:
            return f.read()
    
    @abstractmethod
    def execute(self, state: MarkmapState) -> dict:
        """åŸ·è¡Œ Agent é‚è¼¯"""
        pass
```

### Optimizer Agent èªçŸ¥æ¨¡çµ„

```python
class OptimizerAgent(BaseAgent):
    """
    å„ªåŒ–è€… Agent - å…·å‚™å®Œæ•´èªçŸ¥èƒ½åŠ›
    
    èªçŸ¥æ¨¡çµ„ï¼š
    1. Planning: è¦åŠƒå„ªåŒ–ç›®æ¨™
    2. Decomposition: ä»»å‹™åˆ†è§£
    3. Reflection: åæ€æ”¹é€²
    4. Memory: è¨˜æ†¶ç®¡ç†
    """
    
    def plan(self, state: MarkmapState) -> dict:
        """
        ğŸ§  è¦åŠƒæ¨¡çµ„
        
        è¼¸å…¥ï¼šç•¶å‰ Markmap, LTM ä¸Šä¸‹æ–‡
        è¼¸å‡ºï¼šå„ªåŒ–è¨ˆåŠƒ
        """
        prompt = self._build_planning_prompt(state)
        response = self.model.invoke([HumanMessage(content=prompt)])
        return {"plan": response.content}
    
    def decompose(self, plan: str) -> List[dict]:
        """
        ğŸ§© ä»»å‹™åˆ†è§£æ¨¡çµ„
        
        å°‡å„ªåŒ–è¨ˆåŠƒåˆ†è§£ç‚ºï¼š
        - ç¯€é»çµæ§‹èª¿æ•´
        - åˆ†é¡å±¤æ¬¡å„ªåŒ–
        - èªç¾©ä¸€è‡´æ€§æª¢æŸ¥
        - å·¥ç¨‹å¯è®€æ€§æå‡
        """
        prompt = self._build_decomposition_prompt(plan)
        response = self.model.invoke([HumanMessage(content=prompt)])
        return self._parse_subtasks(response.content)
    
    def reflect(self, previous_results: List[dict], state: MarkmapState) -> dict:
        """
        ğŸ” åæ€æ¨¡çµ„
        
        è©•ä¼°å‰ä¸€è¼ªçµæœï¼Œèª¿æ•´ç­–ç•¥
        """
        prompt = self._build_reflection_prompt(previous_results, state)
        response = self.model.invoke([HumanMessage(content=prompt)])
        return {"reflection": response.content}
    
    def execute(self, state: MarkmapState, other_opinions: List[str]) -> dict:
        """
        åŸ·è¡Œå®Œæ•´å„ªåŒ–æµç¨‹
        
        1. å¾ LTM æª¢ç´¢ç›¸é—œæ±ºç­–
        2. è¦åŠƒ
        3. åˆ†è§£ä»»å‹™
        4. åæ€ï¼ˆéé¦–è¼ªï¼‰
        5. åŸ·è¡Œå„ªåŒ–
        6. æ›´æ–°è¨˜æ†¶
        """
        # 1. LTM æª¢ç´¢
        ltm_context = query_ltm(state["current_markmaps"][0][:500])
        
        # 2. è¦åŠƒ
        plan = self.plan(state)
        
        # 3. åˆ†è§£
        subtasks = self.decompose(plan["plan"])
        
        # 4. åæ€ï¼ˆéé¦–è¼ªï¼‰
        if state["current_round"] > 0:
            reflection = self.reflect(state["round_summaries"], state)
        
        # 5. åŸ·è¡Œå„ªåŒ–
        optimized = self._optimize(state, other_opinions, subtasks)
        
        # 6. æ›´æ–°è¨˜æ†¶
        update_stm(state["stm"], optimized)
        store_to_ltm(optimized)
        
        return optimized
```

---

## è¨˜æ†¶ç³»çµ±æ¶æ§‹

### çŸ­æœŸè¨˜æ†¶ (STM)

```python
class ShortTermMemory:
    """
    çŸ­æœŸè¨˜æ†¶ - ç¶­è­·ç•¶å‰æœƒè©±ä¸Šä¸‹æ–‡
    
    ç‰¹é»ï¼š
    - In-memory å¯¦ä½œ
    - FIFO æ·˜æ±°ç­–ç•¥
    - å¿«é€Ÿå­˜å–
    """
    
    def __init__(self, max_items: int = 50):
        self.max_items = max_items
        self.memory: List[dict] = []
    
    def add(self, item: dict) -> None:
        self.memory.append({
            "timestamp": datetime.now().isoformat(),
            "content": item
        })
        if len(self.memory) > self.max_items:
            self.memory.pop(0)  # FIFO
    
    def get_recent(self, n: int = 10) -> List[dict]:
        return self.memory[-n:]
    
    def search(self, keyword: str) -> List[dict]:
        return [m for m in self.memory if keyword in str(m["content"])]
```

### é•·æœŸè¨˜æ†¶ (LTM)

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

class LongTermMemory:
    """
    é•·æœŸè¨˜æ†¶ - è·¨æœƒè©±æŒä¹…åŒ–
    
    ç‰¹é»ï¼š
    - Vector Store å¯¦ä½œ
    - èªç¾©æœå°‹
    - æŒä¹…åŒ–å­˜å„²
    """
    
    def __init__(self, config: dict):
        self.embeddings = OpenAIEmbeddings(
            model=config["embedding_model"]
        )
        self.vectorstore = Chroma(
            collection_name=config["collection_name"],
            embedding_function=self.embeddings,
            persist_directory=config["chromadb"]["persist_directory"]
        )
    
    def store(self, content: str, metadata: dict = None) -> None:
        """å­˜å„²æ±ºç­–åˆ° LTM"""
        self.vectorstore.add_texts(
            texts=[content],
            metadatas=[metadata or {}]
        )
    
    def query(self, query: str, k: int = 5) -> List[str]:
        """èªç¾©æœå°‹ç›¸é—œæ±ºç­–"""
        docs = self.vectorstore.similarity_search(query, k=k)
        return [doc.page_content for doc in docs]
```

### è¨˜æ†¶æ•´åˆæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       è¨˜æ†¶ç³»çµ±æµç¨‹                               â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚   Agent     â”‚ â”€â”€â”€ æŸ¥è©¢ç›¸é—œæ±ºç­– â”€â”€â”€â”€â–º â”‚    LTM      â”‚       â”‚
â”‚   â”‚             â”‚ â—„â”€â”€ è¿”å›ä¸Šä¸‹æ–‡ â”€â”€â”€â”€â”€â”€â”€ â”‚  (Vector)   â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚          â”‚                                                      â”‚
â”‚          â”‚ åŸ·è¡Œæ±ºç­–                                              â”‚
â”‚          â–¼                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚   Result    â”‚ â”€â”€â”€ å­˜å…¥çŸ­æœŸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚    STM      â”‚       â”‚
â”‚   â”‚             â”‚                         â”‚  (Memory)   â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚          â”‚                                       â”‚              â”‚
â”‚          â”‚ é‡è¦æ±ºç­–                               â”‚ æœƒè©±çµæŸ    â”‚
â”‚          â–¼                                       â–¼              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚   Store to  â”‚ â—„â”€â”€ æŒä¹…åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   Persist   â”‚       â”‚
â”‚   â”‚    LTM      â”‚                         â”‚    STM      â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## éŒ¯èª¤è™•ç†ç­–ç•¥

### é‡è©¦æ©Ÿåˆ¶

```python
from tenacity import retry, stop_after_attempt, wait_exponential

class RobustLLMCall:
    """å¸¶é‡è©¦çš„ LLM å‘¼å«"""
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10)
    )
    def invoke(self, messages: List[dict]) -> str:
        try:
            return self.model.invoke(messages)
        except RateLimitError:
            logger.warning("Rate limit hit, retrying...")
            raise
        except APIError as e:
            logger.error(f"API error: {e}")
            raise
```

### Checkpoint æ¢å¾©

```python
def resume_from_checkpoint(thread_id: str):
    """å¾ Checkpoint æ¢å¾©åŸ·è¡Œ"""
    graph = build_graph()
    
    # å–å¾—æœ€æ–° checkpoint
    state = graph.get_state({"configurable": {"thread_id": thread_id}})
    
    if state.values:
        logger.info(f"Resuming from round {state.values['current_round']}")
        return graph.invoke(None, {"configurable": {"thread_id": thread_id}})
    else:
        logger.warning("No checkpoint found, starting fresh")
        return None
```

---

## æ•ˆèƒ½å„ªåŒ–

### 1. ä¸¦è¡ŒåŸ·è¡Œ

```python
# ç¬¬ä¸€éšæ®µï¼š4 å€‹ç”Ÿæˆå™¨ä¸¦è¡Œ
graph.add_edge(START, "gen_general_en")
graph.add_edge(START, "gen_general_zh")
graph.add_edge(START, "gen_specialist_en")
graph.add_edge(START, "gen_specialist_zh")

# LangGraph è‡ªå‹•ä¸¦è¡ŒåŸ·è¡Œç„¡ä¾è³´çš„ç¯€é»
```

### 2. å…§å®¹å£“ç¸®

```python
def compress_if_needed(state: MarkmapState) -> dict:
    """æ™ºæ…§å£“ç¸® - åƒ…åœ¨å¿…è¦æ™‚å£“ç¸®"""
    
    estimated_tokens = estimate_tokens(state["discussion_history"])
    threshold = config["workflow"]["max_tokens_before_compress"]
    
    if estimated_tokens > threshold:
        compressed = compress_content(state["discussion_history"])
        return {"compressed_discussion": compressed}
    
    return {}  # ä¸å£“ç¸®
```

### 3. å¿«å–ç­–ç•¥

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_embedding(text: str) -> List[float]:
    """å¿«å– embedding çµæœ"""
    return embeddings.embed_query(text)
```

### 4. ä¸²æµè¼¸å‡º

```python
async def stream_optimization(state: MarkmapState):
    """ä¸²æµè¼¸å‡ºå„ªåŒ–éç¨‹"""
    async for event in graph.astream(state):
        yield event
```

---

## é™„éŒ„ï¼šè¨­è¨ˆæ±ºç­–è¨˜éŒ„

| æ±ºç­– | é¸é … | é¸æ“‡ | åŸå›  |
|------|------|------|------|
| ç‹€æ…‹ç®¡ç† | Redux / Zustand / LangGraph State | LangGraph State | èˆ‡ Graph ç·Šå¯†æ•´åˆ |
| Vector Store | Chroma / Pinecone / FAISS | Chroma | å…è²»ã€æœ¬åœ°ã€æ˜“éƒ¨ç½² |
| é…ç½®æ ¼å¼ | JSON / YAML / TOML | YAML | å¯è®€æ€§å¥½ã€æ”¯æ´è¨»è§£ |
| æ—¥èªŒæ¡†æ¶ | logging / loguru | loguru | æ›´å¥½çš„æ ¼å¼åŒ– |

---

*Last updated: 2024-12*


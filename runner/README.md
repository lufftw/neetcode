# Test Runner - Quick Reference

> **Quick Start Guide**  
> For complete documentation, see [docs/runner/README.md](../docs/runner/README.md)

## Overview

`test_runner.py` is the main test execution engine for running solutions against test cases, comparing multiple solution approaches, and generating random test cases.

## Quick Start

```bash
# Run default solution
python runner/test_runner.py 0001_two_sum

# Run specific solution method
python runner/test_runner.py 0023 --method heap

# Test all solutions with benchmark
python runner/test_runner.py 0023 --all --benchmark

# Generate random test cases
python runner/test_runner.py 0004 --generate 10 --seed 12345
```

## Common Commands

| Command | Description |
|---------|-------------|
| `python runner/test_runner.py <problem>` | Run default solution |
| `python runner/test_runner.py <problem> --method <name>` | Run specific method |
| `python runner/test_runner.py <problem> --all` | Test all solutions |
| `python runner/test_runner.py <problem> --all --benchmark` | Compare performance |
| `python runner/test_runner.py <problem> --all --benchmark --estimate` | Compare with complexity estimation |
| `python runner/test_runner.py <problem> --generate N` | Generate N test cases |
| `python runner/test_runner.py <problem> --generate N --seed S` | Reproducible generation |
| `python runner/test_runner.py <problem> --estimate` | Estimate complexity |
| `python runner/test_runner.py <problem> --all --memory-trace` | Show memory traces |
| `python runner/test_runner.py <problem> --all --trace-compare` | Compare memory usage |
| `python runner/test_runner.py <problem> --memory-per-case` | Debug: Top-K cases by RSS |

## Key Features

- ‚úÖ **Multi-Solution Testing**: Test all solution variants in one run
- ‚úÖ **Performance Benchmarking**: Compare execution times with visual bar chart
- ‚úÖ **Random Test Generation**: Stress testing with seed support
- ‚úÖ **Custom Validation**: JUDGE_FUNC or COMPARE_MODE
- ‚úÖ **Complexity Estimation**: Empirical Big-O analysis
- ‚úÖ **Memory Profiling**: RSS measurement and comparison (requires `psutil`)

## Visual Performance Comparison

When running multiple solutions with `--all --benchmark`, the test runner displays a visual bar chart with approach names extracted from class comments:

```
   ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
   ‚ïë                  0131_palindrome_partitioning - Performance                   ‚ïë
   ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
   ‚ïë default: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  158ms                                          ‚ïë
   ‚ïë naive:   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë  152ms                                          ‚ïë
   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

   default  ‚Üí Backtracking with DP-Precomputed Palindrome Table
   naive    ‚Üí Backtracking with On-the-Fly Checking
```

### Integrated Complexity Estimation

Use `--all --benchmark --estimate` to include estimated complexity in the visual charts:

```bash
python runner/test_runner.py 0215 --all --benchmark --estimate
```

Output includes estimated complexity alongside approach descriptions:

```
   ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
   ‚ïë 0215_kth_largest_element_in_an_array - Performance ‚ïë
   ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
   ‚ïë default:     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë  170ms           ‚ïë
   ‚ïë quickselect: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë  191ms           ‚ïë
   ‚ïë heap:        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  199ms           ‚ïë
   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

   default      ‚Üí Quickselect Algorithm
   quickselect  ‚Üí Quickselect Algorithm
   heap         ‚Üí Heap-Based Solution

   üìà Estimated Complexity:
   default    : O(n)        [confidence: 1.00]
   quickselect: O(n log n)  [confidence: 1.00]
   heap       : O(n log n)  [confidence: 1.00]
```

The detailed table also shows both declared and estimated complexity:

```
======================================================================
Performance Comparison (Details)
======================================================================

Method         Avg Time   Pass Rate      Declared     Estimated
-----------  ----------  ----------  ------------  ------------
default        169.59ms         3/3  O(n) average          O(n)
quickselect    190.74ms         3/3  O(n) average    O(n log n)
heap           198.63ms         3/3  O(n log k)      O(n log n)
======================================================================
```

> **Note:** Complexity estimation requires `generate_for_complexity(n)` function in the generator and `pip install big-O`.

## Memory Profiling

The `--benchmark` flag automatically includes memory metrics (Peak RSS, P95 RSS) in the comparison table when `psutil` is installed.

```bash
# Memory metrics in benchmark table
python runner/test_runner.py 0023 --all --benchmark

# Run-level memory traces
python runner/test_runner.py 0023 --all --memory-trace

# Multi-method memory comparison with ranking
python runner/test_runner.py 0023 --all --trace-compare

# Debug: Top 5 cases by peak RSS
python runner/test_runner.py 0023 --memory-per-case
```

**Benchmark Table with Memory Columns:**

```
Method     Avg Time   Pass Rate  Aux Space  Peak RSS   P95 RSS
default      83.2ms     50/50     O(N)       25.4MB     23.1MB
native      120.5ms     50/50     O(1)       21.1MB     20.8MB
```

**Memory Trace Output:**

```
Memory Trace (Run-level RSS)

default:
‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ
Peak 25.4MB | P95 23.1MB
```

> **Note:** Memory profiling requires `pip install psutil`. The system gracefully degrades without it.

**Enhanced Method Display:**

Each method also shows detailed information when running:

```
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üìå Shorthand: default
   Approach: Backtracking with DP-Precomputed Palindrome Table
   Complexity: O(n √ó 2^n) time, O(n^2) space
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

The approach names are automatically parsed from the class header comments in the solution file:

```python
# ============================================================================
# Solution 1: Backtracking with DP-Precomputed Palindrome Table
# Time: O(n √ó 2^n), Space: O(n^2)
#   - Key insight or implementation detail
# ============================================================================
class SolutionDP:
    ...
```

**Usage:**
```bash
python runner/test_runner.py 0131 --all --benchmark
python runner/test_runner.py 0215 --all --benchmark --estimate
```

> **Note:** On terminals that don't support Unicode (e.g., some Windows terminals), ASCII fallback characters are used automatically.

## File Structure

```
runner/
‚îú‚îÄ‚îÄ test_runner.py          # Main CLI entry point (~380 lines)
‚îú‚îÄ‚îÄ module_loader.py         # Load solution/generator modules (~120 lines)
‚îú‚îÄ‚îÄ executor.py              # Execute test cases (~240 lines)
‚îú‚îÄ‚îÄ method_runner.py         # Run tests per method (~410 lines)
‚îú‚îÄ‚îÄ reporter.py              # Format results (~440 lines)
‚îú‚îÄ‚îÄ compare.py               # Output validation (~190 lines)
‚îú‚îÄ‚îÄ complexity_estimator.py  # Big-O estimation (~290 lines)
‚îú‚îÄ‚îÄ memory_profiler.py       # RSS measurement and metrics (~340 lines)
‚îî‚îÄ‚îÄ solution_parser.py       # Parse solution class comments (~250 lines)
```

## Requirements

- Solution files in `solutions/` (see [Solution Contract](../docs/SOLUTION_CONTRACT.md))
- Test files in `tests/` (optional, can use generators)
- Generator files in `generators/` (optional, for random testing)
- `big-O` package (optional, for complexity estimation): `pip install big-O`
- `psutil` package (optional, for memory profiling): `pip install psutil`

## Validation Modes

| Mode | When Used |
|------|-----------|
| `judge` | JUDGE_FUNC + `.out` exists |
| `judge-only` | JUDGE_FUNC, no `.out` (generated tests) |
| `exact` | COMPARE_MODE="exact" |
| `sorted` | COMPARE_MODE="sorted" |
| `set` | COMPARE_MODE="set" |
| `skip` | No `.out`, no JUDGE_FUNC |

## Troubleshooting

**No test files found?**
- Create `tests/{problem}_*.in` files, or
- Use `--generate N` to generate test cases

**Generator requires JUDGE_FUNC?**
- Add `JUDGE_FUNC` to solution file, or
- Use static tests only (without `--generate`)

**Method not found?**
- Check `SOLUTIONS` dict in solution file
- List available methods: run without `--method` to see error

## Related Documentation

- **[Complete Documentation](../docs/runner/README.md)**: Full specification and examples
- **[Solution Contract](../docs/SOLUTION_CONTRACT.md)**: Solution file requirements
- **[Generator Contract](../docs/GENERATOR_CONTRACT.md)**: Generator file requirements

---

## Documentation Maintenance

‚ö†Ô∏è **Important:** When modifying `test_runner.py` or its behavior:

1. **Update this README** (`runner/README.md`) - Quick reference changes
2. **Update detailed docs** (`docs/runner/README.md`) - Complete specification
3. **Update docstring** (`test_runner.py`) - Inline documentation

These three files must stay in sync. See [Complete Documentation](../docs/runner/README.md) for the full specification.


# runner/test_runner.py
"""
Test Runner - æ”¯æ´å¤šè§£æ³•æ¸¬è©¦èˆ‡æ•ˆèƒ½æ¯”è¼ƒ
Usage:
    python runner/test_runner.py 0001_two_sum                    # åŸ·è¡Œé è¨­è§£æ³•
    python runner/test_runner.py 0023 --method heap              # åŸ·è¡ŒæŒ‡å®šè§£æ³•
    python runner/test_runner.py 0023 --all                      # åŸ·è¡Œæ‰€æœ‰è§£æ³•
    python runner/test_runner.py 0023 --all --benchmark          # æ‰€æœ‰è§£æ³• + æ•ˆèƒ½æ¯”è¼ƒ
"""
import subprocess
import glob
import os
import sys
import time
import argparse
import importlib.util
from typing import Optional, Dict, List, Any

PYTHON_EXE = sys.executable


def normalize_output(s: str) -> str:
    """æ­£è¦åŒ–è¼¸å‡ºï¼Œé¿å…å¤šé¤˜ç©ºç™½/æ›è¡Œé€ æˆæ¯”å°å¤±æ•—ã€‚"""
    lines = s.strip().splitlines()
    lines = [line.rstrip() for line in lines]
    return "\n".join(lines)


def load_solution_module(problem: str):
    """å‹•æ…‹è¼‰å…¥ solution æ¨¡çµ„ï¼Œå–å¾— SOLUTIONS metadata"""
    solution_path = os.path.join("solutions", f"{problem}.py")
    if not os.path.exists(solution_path):
        return None, None
    
    spec = importlib.util.spec_from_file_location(f"solution_{problem}", solution_path)
    module = importlib.util.module_from_spec(spec)
    try:
        spec.loader.exec_module(module)
    except Exception as e:
        print(f"âš ï¸ è¼‰å…¥æ¨¡çµ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None, None
    
    # å–å¾— SOLUTIONS metadataï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    solutions_meta = getattr(module, 'SOLUTIONS', None)
    return module, solutions_meta


def run_one_case(problem: str, input_path: str, output_path: str, 
                 method: Optional[str] = None, benchmark: bool = False) -> tuple[bool, float]:
    """
    åŸ·è¡Œå–®ä¸€æ¸¬è³‡
    Returns: (passed: bool, elapsed_ms: float)
    """
    with open(input_path, "r", encoding="utf-8") as f:
        input_data = f.read()
    
    with open(output_path, "r", encoding="utf-8") as f:
        expected = f.read()
    
    solution_path = os.path.join("solutions", f"{problem}.py")
    if not os.path.exists(solution_path):
        print(f"âŒ æ‰¾ä¸åˆ°è§£ç­”æª”æ¡ˆ: {solution_path}")
        return False, 0.0
    
    # æº–å‚™ç’°å¢ƒè®Šæ•¸å‚³é method åƒæ•¸
    env = os.environ.copy()
    if method:
        env['SOLUTION_METHOD'] = method
    
    start_time = time.perf_counter()
    result = subprocess.run(
        [PYTHON_EXE, solution_path],
        input=input_data,
        text=True,
        capture_output=True,
        env=env
    )
    elapsed_ms = (time.perf_counter() - start_time) * 1000
    
    actual = result.stdout
    exp_norm = normalize_output(expected)
    act_norm = normalize_output(actual)
    
    ok = (exp_norm == act_norm)
    return ok, elapsed_ms


def run_method_tests(problem: str, method_name: str, method_info: Dict[str, Any],
                     input_files: List[str], benchmark: bool = False) -> Dict[str, Any]:
    """åŸ·è¡ŒæŸå€‹è§£æ³•çš„æ‰€æœ‰æ¸¬è³‡"""
    results = {
        "method": method_name,
        "display_name": method_info.get("method", method_name),
        "complexity": method_info.get("complexity", "Unknown"),
        "description": method_info.get("description", ""),
        "cases": [],
        "passed": 0,
        "total": 0,
        "times": []
    }
    
    print(f"\nğŸ“Œ Method: {method_name}")
    if method_info.get("complexity"):
        print(f"   Complexity: {method_info['complexity']}")
    if method_info.get("description"):
        print(f"   Description: {method_info['description']}")
    print()
    
    for in_path in input_files:
        out_path = in_path.replace(".in", ".out")
        if not os.path.exists(out_path):
            print(f"   âš ï¸ æ‰¾ä¸åˆ°å°æ‡‰çš„ output æª”: {out_path}")
            continue
        
        case_name = os.path.basename(in_path).replace(".in", "")
        ok, elapsed_ms = run_one_case(problem, in_path, out_path, method_name, benchmark)
        
        results["total"] += 1
        results["times"].append(elapsed_ms)
        
        if ok:
            results["passed"] += 1
            if benchmark:
                print(f"   {case_name}: âœ… PASS ({elapsed_ms:.2f}ms)")
            else:
                print(f"   {case_name}: âœ… PASS")
        else:
            print(f"   {case_name}: âŒ FAIL")
        
        results["cases"].append({
            "name": case_name,
            "passed": ok,
            "time_ms": elapsed_ms
        })
    
    return results


def print_benchmark_summary(all_results: List[Dict[str, Any]]):
    """å°å‡ºæ•ˆèƒ½æ¯”è¼ƒè¡¨"""
    print("\n" + "=" * 60)
    print("ğŸ“Š Performance Comparison")
    print("=" * 60)
    
    # è¡¨é ­
    print(f"{'Method':<20} {'Avg Time':<12} {'Complexity':<15} {'Pass Rate'}")
    print("-" * 60)
    
    for result in all_results:
        method = result["method"]
        complexity = result["complexity"]
        avg_time = sum(result["times"]) / len(result["times"]) if result["times"] else 0
        pass_rate = f"{result['passed']}/{result['total']}"
        
        print(f"{method:<20} {avg_time:>8.2f}ms   {complexity:<15} {pass_rate}")
    
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="LeetCode Test Runner - æ”¯æ´å¤šè§£æ³•æ¸¬è©¦",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python runner/test_runner.py 0001_two_sum
  python runner/test_runner.py 0023 --method heap
  python runner/test_runner.py 0023 --all
  python runner/test_runner.py 0023 --all --benchmark
        """
    )
    parser.add_argument("problem", help="é¡Œç›®åç¨± (e.g., 0001_two_sum)")
    parser.add_argument("--method", "-m", help="æŒ‡å®šè¦æ¸¬è©¦çš„è§£æ³•åç¨±")
    parser.add_argument("--all", "-a", action="store_true", help="æ¸¬è©¦æ‰€æœ‰è§£æ³•")
    parser.add_argument("--benchmark", "-b", action="store_true", help="é¡¯ç¤ºåŸ·è¡Œæ™‚é–“æ¯”è¼ƒ")
    parser.add_argument("--tests-dir", "-t", default="tests", help="æ¸¬è³‡ç›®éŒ„ (é è¨­: tests)")
    
    args = parser.parse_args()
    
    problem = args.problem
    tests_dir = args.tests_dir
    
    # æ‰¾æ¸¬è³‡æª”æ¡ˆ
    pattern = os.path.join(tests_dir, f"{problem}_*.in")
    input_files = sorted(glob.glob(pattern))
    if not input_files:
        print(f"âš ï¸ æ‰¾ä¸åˆ°æ¸¬è³‡æª”æ¡ˆ (no test inputs): {pattern}")
        sys.exit(1)
    
    # è¼‰å…¥ solution æ¨¡çµ„å–å¾— SOLUTIONS metadata
    module, solutions_meta = load_solution_module(problem)
    
    print(f"\n{'=' * 60}")
    print(f"ğŸ§ª Testing: {problem}")
    print(f"{'=' * 60}")
    
    # æ±ºå®šè¦æ¸¬è©¦å“ªäº›è§£æ³•
    if args.all and solutions_meta:
        # æ¸¬è©¦æ‰€æœ‰è§£æ³•
        methods_to_test = list(solutions_meta.keys())
    elif args.method:
        # æ¸¬è©¦æŒ‡å®šè§£æ³•
        methods_to_test = [args.method]
    elif solutions_meta and "default" in solutions_meta:
        # æœ‰ SOLUTIONS ä½†æ²’æŒ‡å®šï¼Œç”¨ default
        methods_to_test = ["default"]
    else:
        # æ²’æœ‰ SOLUTIONS metadataï¼Œä½¿ç”¨å‚³çµ±æ¨¡å¼
        methods_to_test = [None]
    
    all_results = []
    
    for method in methods_to_test:
        if method is None:
            # å‚³çµ±æ¨¡å¼ï¼šä¸æŒ‡å®š method
            print(f"\nğŸ“Œ Running default solution...")
            print()
            passed = 0
            total = 0
            times = []
            
            for in_path in input_files:
                out_path = in_path.replace(".in", ".out")
                if not os.path.exists(out_path):
                    print(f"   âš ï¸ æ‰¾ä¸åˆ°å°æ‡‰çš„ output æª”: {out_path}")
                    continue
                
                case_name = os.path.basename(in_path).replace(".in", "")
                ok, elapsed_ms = run_one_case(problem, in_path, out_path, None, args.benchmark)
                total += 1
                times.append(elapsed_ms)
                
                if ok:
                    passed += 1
                    if args.benchmark:
                        print(f"   {case_name}: âœ… PASS ({elapsed_ms:.2f}ms)")
                    else:
                        print(f"   {case_name}: âœ… PASS")
                else:
                    print(f"   {case_name}: âŒ FAIL")
            
            print(f"\næ¸¬è©¦çµæœ / Summary: {passed} / {total} cases passed.")
            
            if args.benchmark and times:
                avg_time = sum(times) / len(times)
                print(f"å¹³å‡åŸ·è¡Œæ™‚é–“ / Avg Time: {avg_time:.2f}ms")
        else:
            # å¤šè§£æ³•æ¨¡å¼
            method_info = solutions_meta.get(method, {"method": method}) if solutions_meta else {"method": method}
            result = run_method_tests(problem, method, method_info, input_files, args.benchmark)
            all_results.append(result)
            print(f"\n   Result: {result['passed']} / {result['total']} cases passed.")
    
    # å¦‚æœæ˜¯å¤šè§£æ³• + benchmarkï¼Œå°å‡ºæ¯”è¼ƒè¡¨
    if len(all_results) > 1 and args.benchmark:
        print_benchmark_summary(all_results)
    elif len(all_results) == 1:
        result = all_results[0]
        print(f"\næ¸¬è©¦çµæœ / Summary: {result['passed']} / {result['total']} cases passed.")


if __name__ == "__main__":
    main()
